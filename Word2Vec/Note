



问题：
	1. U0V是从context representation matrix中抽取一列吗？对应元素相乘然后组成了向量形式？这是点乘吗
	2. 虽然在求softmax的时候确实是分子是这列向量的每一维度除以corpus的 expotional？
	

http://www.cnblogs.com/pinard/p/7243513.html
http://qiancy.com/2016/08/17/word2vec-hierarchical-softmax/
天然向量：
	图片：像素点的稠密矩阵
	音频：声音讯号频谱

离散单独符号：
	自然语言
	one-hot
	每个词是一个向量
	整篇文章是一个稀疏矩阵
Bag of word：将每个词对应的向量加在一起，一个向量，每一位是字的出现个数
one-hot缺点：字符编码是随机的，没有提供字词之间的关联关系；稀疏矩阵训练计算麻烦
解决：向量空间模型将离散编码转化为连续值的向量表达，其中意思相近的词被映射到空间中相近的位置

	
	1. 计数模型，统计语料库中相邻词出现的频率，统计结果转化为小而稠密矩阵
	2. 预测模型，通过中间词预测周围上下文，使用极大似然的方法，给定上下文的情况下，最大化（目标词）出现概率



word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？

Word2vec：CBOW 
Skip-gram
添加噪声的方法训练，二分类，是否为目标词汇。
skip-gram在大型语料中表现比较好。
效果：1. 近义词在空间中位置相近
	2. 学习到了词与词之间的关系，词语是空间中的一个点，可以计算点到点的向量（男人->女人，进行时->过去时）

普遍认为Hierarchical Softmax对低频词效果较好；Negative Sampling对高频词效果较好，向量维度较低时效果更好。
word2vec基础之霍夫曼树
在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。
权重越大，频数越大

那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。

目标是出现概率高的符号使用较短的编码，出现概率低的符号则使用较长的编码。这种方式能有效降低编码之后的字符串的平均长度。在谈论数据结构的时候，Huffman编码又称为最优二叉树，表示一种带权路径长度最短的二叉树。带权路径长度，指的就是叶子结点的权值乘以该结点到根结点的路径长度。

在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。


基于Hierarchical Softmax的模型概述

word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。


和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为"Hierarchical Softmax"。
如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即

其中xwxw是当前内部节点的词向量，而θθ则是我们需要从训练样本求出的逻辑回归的模型参数。
使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为VV,现在变成了log2Vlog2V。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。
容易理解，被划分为左子树而成为负类的概率为P(−)=1−P(+)P(−)=1−P(+)。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看P(−),P(+)P(−),P(+)谁的概率值大。而控制P(−),P(+)P(−),P(+)谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数θθ。



基于Negative Sampling的模型概述

但是如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了
比如我们有一个训练样本，中心词是w,它周围上下文共有2c2c个词，记为context(w)context(w)。由于这个中心词ww,的确和context(w)context(w)相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和ww不同的中心词wi,i=1,2,..negwi,i=1,2,..neg，这样context(w)context(w)和$$w_i$就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词$w_i$对应的模型参数$\theta_{i}$，和每个词的词向量。
从上面的描述可以看出，Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。

Noise Contrastive Estimation(NCE)



TensorFlow实战：
	1. 建立vocabulary词汇表，统计单词频数，取前top 50000放入字典
	2. NCE loss

https://www.linkedin.com/pulse/heavy-softmax-use-nce-loss-shamane-siriwardhana
