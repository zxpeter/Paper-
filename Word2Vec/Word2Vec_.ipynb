{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0HXUu4glTTuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import urllib\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2rnUv1J416S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0e5789a-1d1e-4090-82aa-1a8f10bfcba6"
      },
      "cell_type": "code",
      "source": [
        "# 从网址下载数据并检查数据的准确性\n",
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "\n",
        "def maybe_download(filename, excepted_bytes):\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == excepted_bytes:\n",
        "        print(\"Found and verified\", filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            \"Failed to verfy\" + filename + \"Can you get to it with browser?\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gRGsYEM6Cpmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8927e88-de39-4789-bc71-337bf21c07f2"
      },
      "cell_type": "code",
      "source": [
        "# 定义读取数据的函数，并把数据转成列表\n",
        "def read_data(filename):\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "\n",
        "words = read_data(filename)\n",
        "print('Data size', len(words))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ntbW1TR1DLZi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 按照词频编号\n",
        "vocabulary_size = 50000\n",
        "\n",
        "def build_dataset(words):\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "        \n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZrnmP1dmGy8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "137f68b6-ba7d-4e9a-97a1-1f398e7741bf"
      },
      "cell_type": "code",
      "source": [
        "del words\n",
        "print(count[:5])\n",
        "print(data[:10], [reverse_dictionary[i] for i in data[:10]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
            "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZkiPzs7qHvmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate batch sample for train\n",
        "data_index = 0\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    span = 2 * skip_window + 1\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    \n",
        "    for _ in range(span):\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = ( data_index + 1) % len(data)\n",
        "    for i in range(batch_size // num_skips):\n",
        "        target = skip_window\n",
        "        targets_to_avoid = [skip_window]\n",
        "        for j in range(num_skips):\n",
        "            while target in targets_to_avoid:\n",
        "                target = random.randint(0, span - 1)\n",
        "            targets_to_avoid.append(target)\n",
        "            batch[i * num_skips + j] = buffer[skip_window]\n",
        "            labels[i * num_skips + j] = buffer[target]\n",
        "            \n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "    return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LepeznR5MMS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "0c7be875-50f1-4e7c-d968-cfaaed4bd7b9"
      },
      "cell_type": "code",
      "source": [
        "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
        "for i in range(8):\n",
        "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i,0]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3081 originated -> 12 as\n",
            "3081 originated -> 5234 anarchism\n",
            "12 as -> 3081 originated\n",
            "12 as -> 6 a\n",
            "6 a -> 195 term\n",
            "6 a -> 12 as\n",
            "195 term -> 2 of\n",
            "195 term -> 6 a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OhMChM7UMmWB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "embedding_size = 128\n",
        "skip_window = 1\n",
        "num_skips = 2\n",
        "valid_size = 16\n",
        "valid_window =100\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "num_sampled = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlmSDyuie9zS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "46234ee1-948c-4c21-b5ab-8fd710c9e37f"
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])    \n",
        "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "    \n",
        "    with tf.device('/cpu:0'):\n",
        "        embeddings = tf.Variable(\n",
        "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
        "        )\n",
        "        \n",
        "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "        \n",
        "        nce_weights = tf.Variable(\n",
        "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                               stddev=1.0 / math.sqrt(embedding_size))\n",
        "        )\n",
        "        \n",
        "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "        \n",
        "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                        biases=nce_biases,\n",
        "                                         labels=train_labels,\n",
        "                                         inputs=embed,\n",
        "                                         num_sampled=num_sampled,\n",
        "                                         num_classes=vocabulary_size\n",
        "                                        ))\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
        "\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
        "    normalized_embeddings = embeddings / norm\n",
        "    valid_embeddings = tf.nn.embedding_lookup(\n",
        "        normalized_embeddings, valid_dataset\n",
        "    )\n",
        "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b = True)\n",
        "\n",
        "    init = tf.global_variables_initializer()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-a7827ece09e8>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NoNlbPRLhmBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4103
        },
        "outputId": "bdfd6690-d92b-4280-d7c7-6627cdb18a42"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 100001\n",
        "with tf.Session(graph=graph) as session:\n",
        "    init.run()\n",
        "    print('Initialized')\n",
        "    \n",
        "    average_loss = 0\n",
        "    for step in range(num_steps):\n",
        "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
        "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
        "        \n",
        "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "        average_loss += loss_val\n",
        "        \n",
        "        if step % 2000 == 0:\n",
        "            if step > 0:\n",
        "                average_loss /= 2000\n",
        "            print('average loss at step', step, ':', average_loss)\n",
        "            average_loss = 0\n",
        "        if step % 10000 == 0:\n",
        "            sim = similarity.eval()\n",
        "            for i in range(valid_size):\n",
        "                valid_word = reverse_dictionary[valid_examples[i]]\n",
        "                top_k = 8\n",
        "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
        "                log_str = 'nearest to %s' % valid_word\n",
        "                for k in range(top_k):\n",
        "                    close_word = reverse_dictionary[nearest[k]]\n",
        "                    log_str = '%s %s,' % (log_str, close_word)\n",
        "                print(log_str)\n",
        "        final_embeddings = normalized_embeddings.eval()\n",
        "        "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "average loss at step 0 : 282.6376647949219\n",
            "nearest to system obscured, viceroyalty, habitat, detected, sidney, karnataka, bypasses, royale,\n",
            "nearest to were professorship, pero, miscarriage, devastates, stopover, qumran, asmara, vibrant,\n",
            "nearest to known helga, alternates, speller, consequently, nen, frege, iala, ligurian,\n",
            "nearest to these reigned, densely, boka, kitchen, plissken, fitness, kyrenia, mothman,\n",
            "nearest to on heckler, hen, fro, oireachtas, carotene, catcher, bwv, warsaw,\n",
            "nearest to can relegated, carinthia, ithilien, tracing, plate, savannas, bef, hausa,\n",
            "nearest to its sravaka, alkane, fughetta, contiguous, frontal, swf, hipc, don,\n",
            "nearest to state petty, governess, fittest, gini, snowboarding, tavistock, andrew, scone,\n",
            "nearest to this unschooling, mindset, whitey, est, confederacy, fibre, disapproved, rosenthal,\n",
            "nearest to many flask, ung, backlash, jabberwocky, mus, ien, moynihan, rolled,\n",
            "nearest to of activision, manslaughter, governance, cryptographers, surveyor, idm, tortoise, accompanying,\n",
            "nearest to was cemented, sachs, neptunian, embarking, andrade, tay, abol, presidents,\n",
            "nearest to would shifts, defamation, view, drivetrain, fares, isc, colliery, uneducated,\n",
            "nearest to or rump, expending, hold, qigong, timecode, belongs, keeper, indictment,\n",
            "nearest to they drummond, produces, fruits, detachments, hawkins, forseti, transfusion, neorealism,\n",
            "nearest to are bachelors, productions, axiom, mathrm, instances, hierarchy, fandango, montage,\n",
            "average loss at step 2000 : 114.10361230945587\n",
            "average loss at step 4000 : 52.283284120082854\n",
            "average loss at step 6000 : 33.477046766757965\n",
            "average loss at step 8000 : 23.566595294713974\n",
            "average loss at step 10000 : 18.07083163332939\n",
            "nearest to system asterism, hosted, source, verb, containing, bounds, canaris, sidney,\n",
            "nearest to were are, and, often, mathematician, truffaut, as, anna, basins,\n",
            "nearest to known vs, urn, subfields, mechanism, killed, antimatter, victoriae, gland,\n",
            "nearest to these altenberg, uses, the, karpov, densely, defenders, austin, corporations,\n",
            "nearest to on in, sadler, wta, two, tarski, launch, fascism, mathbf,\n",
            "nearest to can cl, rockets, alchemical, plate, antonio, carinthia, nations, brown,\n",
            "nearest to its the, UNK, alkane, don, a, austin, vs, college,\n",
            "nearest to state petty, sagan, knowledge, mulcahy, gland, archbishop, founders, andrew,\n",
            "nearest to this a, the, est, fao, mathbf, confederacy, foster, marginal,\n",
            "nearest to many victoriae, wednesday, the, cl, painter, because, some, liability,\n",
            "nearest to of in, and, for, eugenicists, gollancz, austin, gland, with,\n",
            "nearest to was is, gland, in, audi, prisoners, dictatorship, martin, canaris,\n",
            "nearest to would canaris, placed, suburbs, view, shifts, mifune, piston, defamation,\n",
            "nearest to or and, cathodes, hold, canaris, UNK, characteristic, globular, of,\n",
            "nearest to they produces, fruits, secured, bennett, not, fermentation, boroughs, constance,\n",
            "nearest to are is, and, were, in, disciplines, buildings, anywhere, atomic,\n",
            "average loss at step 12000 : 14.209374116778374\n",
            "average loss at step 14000 : 11.784655007719994\n",
            "average loss at step 16000 : 9.987248501062393\n",
            "average loss at step 18000 : 8.593140124797822\n",
            "average loss at step 20000 : 7.729438202619552\n",
            "nearest to system asterism, dasyprocta, detected, hosted, canaris, containing, contrary, obscured,\n",
            "nearest to were are, was, is, have, as, by, hospitals, often,\n",
            "nearest to known urn, subfields, mechanism, vs, deviation, isu, projections, marginal,\n",
            "nearest to these the, altenberg, reigned, uses, which, agouti, kitchen, densely,\n",
            "nearest to on in, two, from, by, for, sadler, and, with,\n",
            "nearest to can could, crassus, alchemical, would, alcott, antonio, nations, rockets,\n",
            "nearest to its the, their, birkenau, his, tcm, a, brokers, slew,\n",
            "nearest to state petty, xiv, gland, sagan, mulcahy, gollancz, affirmed, passages,\n",
            "nearest to this the, it, a, fao, peptide, est, his, agassiz,\n",
            "nearest to many some, the, victoriae, wednesday, docked, rolled, two, because,\n",
            "nearest to of and, in, for, archie, gollancz, nine, s, gland,\n",
            "nearest to was is, were, by, are, had, in, dasyprocta, as,\n",
            "nearest to would shifts, to, can, suburbs, canaris, placed, centimeters, standoff,\n",
            "nearest to or and, dasyprocta, cathodes, UNK, canaris, five, hold, agouti,\n",
            "nearest to they not, fruits, there, it, which, produces, bennett, he,\n",
            "nearest to are is, were, was, in, and, have, buildings, dasyprocta,\n",
            "average loss at step 22000 : 7.207919632077217\n",
            "average loss at step 24000 : 6.95465246284008\n",
            "average loss at step 26000 : 6.674781816482544\n",
            "average loss at step 28000 : 6.1799395695924755\n",
            "average loss at step 30000 : 6.1593441698551175\n",
            "nearest to system detected, asterism, dasyprocta, amalthea, obscured, hosted, argonauts, succeeds,\n",
            "nearest to were are, was, is, have, by, hospitals, often, truffaut,\n",
            "nearest to known urn, such, subfields, nonprofit, deviation, mechanism, marginal, trinomial,\n",
            "nearest to these which, altenberg, uses, reigned, agouti, corporations, some, many,\n",
            "nearest to on in, two, from, for, by, and, sadler, into,\n",
            "nearest to can would, could, will, to, ketone, alchemical, aba, crassus,\n",
            "nearest to its the, their, his, birkenau, a, tcm, brokers, slew,\n",
            "nearest to state petty, xiv, gland, gollancz, ansgar, sagan, affirmed, mulcahy,\n",
            "nearest to this it, the, a, that, fao, which, peptide, foster,\n",
            "nearest to many some, most, the, wednesday, victoriae, confessed, several, domains,\n",
            "nearest to of and, in, for, s, from, eight, nine, zero,\n",
            "nearest to was is, were, had, by, been, has, dasyprocta, are,\n",
            "nearest to would can, to, will, ethylene, primigenius, suburbs, shifts, centimeters,\n",
            "nearest to or and, dasyprocta, s, peptide, six, agouti, five, gland,\n",
            "nearest to they there, not, he, it, fruits, produces, which, bennett,\n",
            "nearest to are were, is, have, was, do, in, dasyprocta, hospitals,\n",
            "average loss at step 32000 : 5.904461459159851\n",
            "average loss at step 34000 : 5.862115472793579\n",
            "average loss at step 36000 : 5.658361404061317\n",
            "average loss at step 38000 : 5.255294609427452\n",
            "average loss at step 40000 : 5.4730938198566434\n",
            "nearest to system increments, asterism, dasyprocta, amalthea, detected, argonauts, succeeds, obscured,\n",
            "nearest to were are, was, have, is, hospitals, by, in, had,\n",
            "nearest to known such, urn, subfields, nonprofit, marginal, deviation, mechanism, projections,\n",
            "nearest to these many, which, some, altenberg, uses, agouti, reigned, corporations,\n",
            "nearest to on in, from, two, at, sadler, gland, for, duma,\n",
            "nearest to can would, could, will, may, to, ketone, aba, must,\n",
            "nearest to its their, the, his, birkenau, tcm, a, rhombic, brokers,\n",
            "nearest to state petty, xiv, mulcahy, affirmed, surname, ansgar, gland, knowledge,\n",
            "nearest to this it, the, which, that, fao, a, peptide, abitibi,\n",
            "nearest to many some, several, most, these, the, victoriae, confessed, wednesday,\n",
            "nearest to of in, gollancz, adhd, gland, and, from, recitative, absalom,\n",
            "nearest to was is, were, had, by, has, been, dasyprocta, are,\n",
            "nearest to would can, will, to, may, centimeters, primigenius, shifts, affray,\n",
            "nearest to or and, dasyprocta, five, six, eight, seven, four, agouti,\n",
            "nearest to they there, he, not, it, fruits, which, produces, agouti,\n",
            "nearest to are were, is, have, was, in, do, while, nine,\n",
            "average loss at step 42000 : 5.298961507320404\n",
            "average loss at step 44000 : 5.2858715857267375\n",
            "average loss at step 46000 : 5.2812901431322095\n",
            "average loss at step 48000 : 5.02982142829895\n",
            "average loss at step 50000 : 5.158439329862595\n",
            "nearest to system increments, detected, argonauts, obscured, asterism, hex, amalthea, dasyprocta,\n",
            "nearest to were are, was, have, is, had, be, by, hospitals,\n",
            "nearest to known such, urn, subfields, nonprofit, marginal, used, deviation, trinomial,\n",
            "nearest to these many, some, which, agouti, altenberg, uses, corporations, the,\n",
            "nearest to on in, from, thibetanus, at, two, sadler, for, into,\n",
            "nearest to can would, could, will, may, must, should, to, ketone,\n",
            "nearest to its their, his, the, birkenau, tcm, rhombic, sint, dasyprocta,\n",
            "nearest to state petty, xiv, surname, mulcahy, affirmed, ansgar, establishing, precision,\n",
            "nearest to this it, which, the, that, fao, pneumonic, botc, a,\n",
            "nearest to many some, several, these, most, the, confessed, other, naaman,\n",
            "nearest to of in, nine, and, for, thibetanus, adhd, including, from,\n",
            "nearest to was is, were, has, had, by, been, are, be,\n",
            "nearest to would can, will, may, to, must, cannot, centimeters, could,\n",
            "nearest to or and, dasyprocta, thibetanus, five, six, seven, eight, than,\n",
            "nearest to they he, there, it, not, fruits, who, anhydride, we,\n",
            "nearest to are were, is, have, was, do, be, nine, lemmy,\n",
            "average loss at step 52000 : 5.176339180469513\n",
            "average loss at step 54000 : 5.111002765417099\n",
            "average loss at step 56000 : 5.058493677616119\n",
            "average loss at step 58000 : 5.09781848013401\n",
            "average loss at step 60000 : 4.967055550396442\n",
            "nearest to system detected, dasyprocta, increments, amalthea, succeeds, asterism, argonauts, obscured,\n",
            "nearest to were are, was, have, had, be, is, hospitals, do,\n",
            "nearest to known such, urn, subfields, used, marginal, nonprofit, deviation, trinomial,\n",
            "nearest to these many, some, which, agouti, uses, altenberg, corporations, crushed,\n",
            "nearest to on in, callithrix, from, thibetanus, at, upon, and, sadler,\n",
            "nearest to can would, could, will, may, must, should, might, ketone,\n",
            "nearest to its their, the, his, birkenau, tcm, michelob, some, her,\n",
            "nearest to state petty, xiv, michelob, ansgar, mulcahy, affirmed, surname, establishing,\n",
            "nearest to this it, which, the, that, pneumonic, fao, michelob, botc,\n",
            "nearest to many some, several, these, most, other, all, confessed, various,\n",
            "nearest to of callithrix, for, in, nine, thibetanus, and, michelob, gland,\n",
            "nearest to was is, were, had, has, by, been, dasyprocta, wct,\n",
            "nearest to would can, will, may, must, to, could, cannot, did,\n",
            "nearest to or and, callithrix, dasyprocta, marmoset, thibetanus, six, five, tamarin,\n",
            "nearest to they there, he, it, not, we, who, michelob, fruits,\n",
            "nearest to are were, is, have, do, be, was, but, lamo,\n",
            "average loss at step 62000 : 4.790179975986481\n",
            "average loss at step 64000 : 4.818006627440453\n",
            "average loss at step 66000 : 4.966408812880516\n",
            "average loss at step 68000 : 4.910436939120292\n",
            "average loss at step 70000 : 4.777303291678429\n",
            "nearest to system detected, asterism, amalthea, increments, argonauts, dasyprocta, obscured, callithrix,\n",
            "nearest to were are, have, was, had, be, do, is, hospitals,\n",
            "nearest to known such, used, urn, subfields, mitral, marginal, nonprofit, trinomial,\n",
            "nearest to these some, many, which, the, agouti, various, those, such,\n",
            "nearest to on in, callithrix, thibetanus, upon, from, at, sadler, through,\n",
            "nearest to can would, could, will, may, must, should, might, to,\n",
            "nearest to its their, the, his, some, a, her, michelob, birkenau,\n",
            "nearest to state petty, xiv, establishing, surname, mulcahy, michelob, ansgar, affirmed,\n",
            "nearest to this which, it, the, that, one, pneumonic, michelob, subkey,\n",
            "nearest to many some, several, these, most, all, other, various, confessed,\n",
            "nearest to of callithrix, upanija, for, including, adhd, in, gland, archie,\n",
            "nearest to was is, has, were, had, by, been, dasyprocta, wct,\n",
            "nearest to would can, will, may, must, could, to, cannot, did,\n",
            "nearest to or and, callithrix, dasyprocta, mitral, tamarin, thibetanus, marmoset, michelob,\n",
            "nearest to they he, there, it, we, not, who, fruits, michelob,\n",
            "nearest to are were, is, have, do, be, including, was, hooked,\n",
            "average loss at step 72000 : 4.809191863894463\n",
            "average loss at step 74000 : 4.778745063036681\n",
            "average loss at step 76000 : 4.882581919789314\n",
            "average loss at step 78000 : 4.801121659040451\n",
            "average loss at step 80000 : 4.804538802504539\n",
            "nearest to system detected, amalthea, argonauts, asterism, dasyprocta, increments, systems, obscured,\n",
            "nearest to were are, was, have, had, be, do, been, is,\n",
            "nearest to known such, used, subfields, urn, marginal, nonprofit, mitral, loki,\n",
            "nearest to these many, some, which, those, such, various, agouti, both,\n",
            "nearest to on in, from, callithrix, thibetanus, through, upon, at, sadler,\n",
            "nearest to can would, could, will, may, must, might, should, cannot,\n",
            "nearest to its their, the, his, her, syntactically, some, tcm, michelob,\n",
            "nearest to state petty, xiv, michelob, ansgar, mulcahy, establishing, murbella, affirmed,\n",
            "nearest to this which, it, the, that, michelob, pneumonic, subkey, one,\n",
            "nearest to many some, several, these, most, all, other, various, confessed,\n",
            "nearest to of callithrix, including, upanija, nine, in, thibetanus, adhd, eight,\n",
            "nearest to was is, were, had, has, been, became, by, be,\n",
            "nearest to would can, will, may, must, could, cannot, to, might,\n",
            "nearest to or and, callithrix, dasyprocta, tamarin, mitral, than, thibetanus, eight,\n",
            "nearest to they he, there, we, it, not, who, you, she,\n",
            "nearest to are were, is, have, including, do, be, lemmy, hooked,\n",
            "average loss at step 82000 : 4.809585743546486\n",
            "average loss at step 84000 : 4.795576672196388\n",
            "average loss at step 86000 : 4.758436214804649\n",
            "average loss at step 88000 : 4.703745371580124\n",
            "average loss at step 90000 : 4.7639034740924835\n",
            "nearest to system amalthea, systems, dasyprocta, asterism, argonauts, detected, increments, callithrix,\n",
            "nearest to were are, was, have, had, be, been, hospitals, including,\n",
            "nearest to known such, used, subfields, marginal, nonprofit, mitral, urn, alternates,\n",
            "nearest to these many, some, which, those, such, various, both, agouti,\n",
            "nearest to on in, upon, callithrix, thibetanus, from, at, through, into,\n",
            "nearest to can would, could, may, will, must, might, should, cannot,\n",
            "nearest to its their, his, the, her, some, syntactically, stray, our,\n",
            "nearest to state petty, mulcahy, establishing, ansgar, xiv, michelob, murbella, precision,\n",
            "nearest to this it, which, the, that, pneumonic, subkey, michelob, one,\n",
            "nearest to many some, several, these, most, all, various, other, both,\n",
            "nearest to of callithrix, including, in, for, upanija, adhd, eight, thibetanus,\n",
            "nearest to was is, were, had, has, been, became, be, wct,\n",
            "nearest to would can, will, may, could, must, cannot, might, to,\n",
            "nearest to or and, callithrix, dasyprocta, mitral, thibetanus, tamarin, than, six,\n",
            "nearest to they he, there, we, it, you, not, she, who,\n",
            "nearest to are were, is, have, including, be, do, these, hooked,\n",
            "average loss at step 92000 : 4.707574932098389\n",
            "average loss at step 94000 : 4.620024431824684\n",
            "average loss at step 96000 : 4.721279783010483\n",
            "average loss at step 98000 : 4.603804515242577\n",
            "average loss at step 100000 : 4.670790567040443\n",
            "nearest to system systems, amalthea, asterism, dasyprocta, increments, detected, asatru, argonauts,\n",
            "nearest to were are, have, was, had, be, including, hospitals, while,\n",
            "nearest to known such, used, subfields, marginal, nonprofit, mitral, urn, trinomial,\n",
            "nearest to these many, some, those, various, such, which, all, the,\n",
            "nearest to on in, upon, callithrix, through, thibetanus, at, from, into,\n",
            "nearest to can would, may, will, could, must, might, should, cannot,\n",
            "nearest to its their, the, his, her, some, syntactically, our, peacocks,\n",
            "nearest to state petty, mulcahy, establishing, ansgar, xiv, michelob, dump, murbella,\n",
            "nearest to this it, which, the, that, pneumonic, subkey, michelob, what,\n",
            "nearest to many some, several, these, various, all, most, both, other,\n",
            "nearest to of callithrix, upanija, including, and, thibetanus, adhd, in, gland,\n",
            "nearest to was is, were, had, has, been, became, wct, be,\n",
            "nearest to would will, can, may, could, must, might, cannot, should,\n",
            "nearest to or and, callithrix, than, dasyprocta, mitral, six, tamarin, thibetanus,\n",
            "nearest to they there, he, we, it, you, she, not, who,\n",
            "nearest to are were, have, is, including, be, do, these, while,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hY89F4ETivjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}